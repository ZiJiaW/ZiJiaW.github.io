<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>一只特立独行的猫</title>
    <link>https://zijiaw.github.io/</link>
    <description>Recent content on 一只特立独行的猫</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 20 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://zijiaw.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>算法拾遗：XGBoost</title>
      <link>https://zijiaw.github.io/posts/a9-xgboost/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a9-xgboost/</guid>
      <description>本文介绍XGBoost的基本思想。
一个包含$K$树棵的集成模型中，每棵树的预测结果是一个实数值（回归树），如果我们把每棵树看作是一个函数$f_k$，则对于样本$x_i$的预测结果为这些树的输出之和： $$\hat y_i=\phi(x_i)=\sum_{k=1}^K f_k(x_i)$$ 我们的目标就是求解最优的$K$棵树。
对于树模型来说，假设有$T$个叶子结点，每棵树会把输入$x$映射到某一个叶子结点$q(x)$上，并输出该叶子节点上预设的值$w_{q(x)}$。上面用$q$来表示一棵树形成的独特映射（将输入映射到叶子结点），而$w$表示叶子结点上的值向量，这二者实际上就代表了一棵树。
现在我们考虑这样一个模型的损失函数： $$L(\phi)=\sum_il(\hat y_i,y_i) + \sum_{k=1}^K \Omega(f_k)$$ 其中第一项为所有样本上的误差之和，比如平方误差；后一项为正则项，用于限制每棵树的复杂度，复杂度越大惩罚越大，其定义为： $$\Omega(f_k)=\gamma T+\frac{1}{2} \lambda ||w||^2$$ 可以看到其实际上为叶子节点数量$T$与权重$w$的平方和的加权和。
我们使用迭代的策略来一棵一棵计算上面的树模型，假设在第$t$轮添加的树对应函数为$f_t$，则在前$t-1$棵树已经确定的情况下，损失函数为： $$L(\phi_t)=\sum_i l(\hat y_i^{(t-1)}+f_t(x_i),y_i)+\Omega(f_t)$$ 其中$\hat y_i^{(t-1)}$为前$t-1$棵树的预测结果。我们需要在第$t$轮计算一棵最小化$L(\phi_t)$的树，贪心地加入到模型中。
上面式中，变量为$f_t(x_i)$，我们将函数$l(\hat y_i^{(t-1)}+f_t(x_i),y_i)$在$\hat y_i^{(t-1)}$处展开到二阶泰勒来近似： $$l(\hat y_i^{(t-1)}+f_t(x_i),y_i)\approx l(\hat y_i^{(t-1)},y_i)+g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)$$ 其中$g_i$和$h_i$分别是损失函数$l(a, y_i)$在$a=\hat y_i^{(t-1)}$处的一阶和二阶导数。
经过近似，我们去掉式中的常量部分，即$l(\hat y_i^{(t-1)},y_i)$，将需最小化的损失函数修改为： $$L(\phi_t)\approx \sum_{i=1}^n[g_if_t(x_i)+\frac{1}{2}h_if_t^2(x_i)]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2$$ 现在我们已经将$f_t$单独拿了出来，现在考虑$\sum_i f_t(x_i)$是什么？
如果$q(x_i)=j$，那么$f_t(x_i)=w_j$，因此假如集合$I_j$是所有被树映射到叶子节点$j$的样本集合，即$I_j={i| q(x_i)=j}$，那么： $$\sum_if_t(x_i)=\sum_{j=1}^T w_j|I_j|$$ 因此损失函数可以变化为： $$L(\phi_t)=\sum_{j=1}^T \big[(\sum_{i\in I_j} g_i)w_j+\frac{1}{2}(\lambda+\sum_{i\in I_j}h_i)w_j^2\big]+\gamma T$$ 上面的式子中，我们可以很快看出每个叶子结点的权值$w_j$的最优取值为： $$w_j^*=-\frac{\sum_{i\in I_j} g_i}{\lambda+\sum_{i\in I_j}h_i}$$ 上式中，$I_j$取决于当前树的结构，而其余部分均为已知的常值，也就是说，如果第$t$棵树的结构确定下来了，那么叶子节点的输出也可以按照上式直接计算出来。
我们将$w$代入损失函数： $$L(\phi_t)=-\frac{1}{2}\sum_{j=1}^T \frac{(\sum_{i\in I_j} g_i)^2}{\lambda+\sum_{i\in I_j}h_i}+\gamma T$$ 上式给出了一个确定结构的回归树加入后整个模型的损失函数，我们需要构造一棵最小化上式的树。
我们考虑迭代方式构造树，从根节点开始尝试对叶子结点增加分支，假如对叶子结点$j$增加分支，对映射到该结点的样本再进行划分，将$I_j$划分为$I_l$和$I_r$，作为新的两个叶子结点，而其余结点没有变化，那么新树的损失函数为： $$L_{new}=\gamma + L +\frac{1}{2}\Big(\frac{(\sum_{i\in I_j} g_i)^2}{\lambda+\sum_{i\in I_j}h_i}-\frac{(\sum_{i\in I_l} g_i)^2}{\lambda+\sum_{i\in I_l}h_i}-\frac{(\sum_{i\in I_r} g_i)^2}{\lambda+\sum_{i\in I_r}h_i}\Big)$$ 显然，我们希望划分后产生的新树的损失函数最小，实际上就是要找到一个最优的划分，使得上式最小化，如果把对应结点的式子看作该结点的score，那么就是需要新结点的score之和最大。如果上式最小化后，$L_{new}&amp;gt;=L$，则显然在结点$j$处不可再划分了。</description>
    </item>
    
    <item>
      <title>算法拾遗：线段树</title>
      <link>https://zijiaw.github.io/posts/a8-segmenttree/</link>
      <pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a8-segmenttree/</guid>
      <description>线段树（Segment Tree）是一种在数组之上维护的树结构（也叫statistic tree），它能够存储一个数组a，支持下面两个操作：
 以$O(lgn)$的时间查询数组上的一个区间a[l:r]内的某种统计值，比如查询区间内的元素之和，或区间内的最小值最大值等。 支持以$O(lgn)$的时间修改数组中的某一元素。  基础思想 本节按照求区间和需求解释如何构建并维护一棵线段树。其描述如下：
 对于数组a[0:n-1]，树的根节点存储整个区间的和，即sum(a[0:n-1])，等价于其维护区间[0:n-1]； 树的任意中间结点，若维护区间[l:r]，则其左子树维护[l:(l+r)/2]，右子树维护区间[(l+r)/2+1:r]； 若l==r，则此为叶子节点，不再分裂。  这些树节点的存储可以借鉴二叉堆的思想，用一个数组t来存各层的树结点：根结点在t[0]，其左孩子位于t[1]，右孩子位于t[2]&amp;hellip;&amp;hellip;以此类推，即t[i]的左孩子为t[2i+1]，右孩子为t[2i+2]。
复杂度分析 树高 显然树的构建可以递归的完成，因为每次分裂均对区间长度做二分处理，因此树的高度在$lgn$这一量级；
数据更新 若需要更新数组a[i]=x，则显而易见的做法是从树根递归向下直到对应a[i]的叶子结点，将受影响的区间和进行修改，而每一层的所有区间是不相交的，仅会有一个区间包含a[i]，因此这一过程时间复杂度$O(lgn)$。
求区间和 求区间a[l:r]的和时，依然是从树根开始递归向下，只是这里可能会出现触发多次递归的情况：
 若当前结点维护的区间[tl:tr]恰好等于[l:r]，则直接返回其和； 若当前区间[tl:tr]被[l:r]的左半区间，或右半区间包含，则递归至对应子树； 若当前区间与左右半区间均有交集，记中间点为m，则依次递归左子树查询区间[l:m]，递归右子树查询区间[m+1,r]，再求和。  我们证明上面的递归过程中访问的结点数量不超过$4lgn$，从而时间复杂度为$O(lgn)$。
实际上，我们只需要证明树的每一层中访问到的结点数量不超过4即可。通过归纳法证明，首先在第一层（树根）显然仅1个结点。其次，对于任意层：
 若访问了2个结点，则下一层最多访问4个结点，因为每个结点最多触发两次递归； 若访问了3或4个结点，则这些结点构成的区间必然是相连的，中间的1个或2个结点必然恰好等于查询的区间，不会触发下一层的递归，仅有最左侧和最右侧的结点可能触发递归，而这也导致下一层不超过4个；  基础实现 对于上面提到的实现，长度$n$的数组，其产生的树高为$\lceil lgn \rceil+1$，因此数组大小至多$4n$。通过递归可以完成所有操作。
class SegmentTree { vector&amp;lt;int&amp;gt; t; int n; int build(int idx, int l, int r, const vector&amp;lt;int&amp;gt;&amp;amp; a) { if (l == r) return t[idx] = a[l]; int mid = (l + r) / 2; return t[idx] = build(2 * idx + 1, l, mid, a) + build(2 * idx + 2, mid + 1, r, a); } void update(int i, int val, int idx, int l, int r) { if (l == r) { t[idx] = val; return; } int mid = (l + r) / 2; if (i &amp;lt;= mid) update(i, val, 2 * idx + 1, l, mid); else update(i, val, 2 * idx + 2, mid + 1, r); t[idx] = t[2 * idx + 1] + t[2 * idx + 2]; } int sum(int l, int r, int idx, int tl, int tr) { if (l == tl &amp;amp;&amp;amp; r == tr) return t[idx]; int mid = (tl + tr) / 2; if (r &amp;lt;= mid) return sum(l, r, 2 * idx + 1, tl, mid); if (l &amp;gt; mid) return sum(l, r, 2 * idx + 2, mid + 1, tr); return sum(l, mid, 2 * idx + 1, tl, mid) + sum(mid + 1, r, 2 * idx + 2, mid + 1, tr); } public: SegmentTree(const vector&amp;lt;int&amp;gt;&amp;amp; a) : t(4 * a.</description>
    </item>
    
    <item>
      <title>Rust中的协程: Future与async/await</title>
      <link>https://zijiaw.github.io/posts/a7-rsfuture/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a7-rsfuture/</guid>
      <description>本文内容来自Writing an OS in Rust博客。
多任务处理 几乎所有操作系统的基本功能都包含多任务处理，即并发执行多个任务的能力（multitasking）。例如，你可能边阅读这篇文章，边打开音乐播放器听歌。即使你只开了浏览器窗口，操作系统肯定也有很多隐藏的进程执行着各种任务（打开任务管理器可以一看究竟）。
虽然看起来所有的任务都并行执行着，但实际上单个CPU核同一时间只能执行一个任务（它并没有分身术）。为了创造这种并行的假象，操作系统快速地切换着当前CPU正在执行的任务，例如在1s中内切换了100次任务，那么100个任务在1s中内都有所进展，现代CPU通常很快，因此在人的感受下，这100个任务就好像是并行执行的一样。
对于多核CPU，多任务并行才是确实存在的。例如，一个8核CPU确实可以并行处理8个任务，然而操作系统的处理与单核类似，依然需要不停切换任务。
通常来说，有两种多任务执行的形式：
 协作式多任务（cooperative multitasking）：当前任务需要主动放弃CPU资源，然后其他任务才可以切换执行。 抢占式多任务（preemptive multitasking）：操作系统可以在任意时间暂停当前任务，强制切换到其他任务执行。  抢占式多任务 抢占式多任务的关键思想是让操作系统来控制切换任务的时机。操作系统通过响应中断，在中断处理函数中执行任务切换的操作（因为在中断的时候操作系统才会重新获取CPU的控制权）。
保存状态 因为任务可能在任意的时间点被打断，可能处在某个函数/算法的中间点，为了在后续恢复它的执行，操作系统必须备份该任务的所有执行状态，包括栈内容，CPU中所有寄存器的值等等。这一过程叫做上下文切换（context switch）。
由于任务的调用栈信息可能非常庞大，实际上每个任务都具有不同的栈空间（即线程），在切换的时候只需要保存记录对应位置的寄存器（包括program counter，以及栈指针）。这样可以降低上下文切换的开销，因为通常来说1秒内会有约100次上下文切换。
优缺点 抢占式调度的主要优点是操作系统能够完全控制每个任务的执行时间，这样可以保证每个任务对CPU时间的占用是公平的，而不需要信任任务之间自己的协作。这非常重要，特别是我们会去执行第三方的程序（通常难以信任），也会有多个用户共享同一系统。
缺点是每个任务需要自己的栈空间。和共享栈空间相比，这会导致一个相对高的内存占用，也会限制同一时刻可以存在的任务数量（例如32位的Linux下每个线程默认分配2MB的栈空间）。另一个缺点是操作系统必须要保存所有寄存器的值，即使当前任务可能只用到了其中的一部分。
抢占式调度与线程是现代操作系统的基石，它们使得我们能够在操作系统上执行各种各样的不可信任的第三方程序。
协作式多任务 在协作式多任务下，系统依赖于每个任务自愿放弃CPU的控制，然后才能够执行别的任务。这使得任务能够自己选择更合适的暂停的时机，例如当它需要等待网络IO的时候。
协作式多任务通常用在语言层面，例如以协程，或者async/await的形式出现，而不是在操作系统层面。实现的思路是由程序员或者编译器在程序中添加yield操作，从而让当前任务放弃CPU，转而执行其他任务。
协作式多任务通常与异步编程相结合，在异步操作中，一个耗时未完成的IO操作通常会返回&amp;quot;not ready&amp;quot;状态，而不会阻塞当前任务，此时任务就可以通过yield将CPU让给其他任务，直到IO操作ready。
保存状态 由于任务自己控制暂停的时机，这样就不依赖于操作系统去保存执行状态了。相对应的，任务可以自行保存其恢复执行需要的信息，这通常带来更好的性能。例如，一个任务刚好结束了一个复杂的算法，那么它可能只需要保存最终的计算结果，而不再需要那些中间值了。
编程语言实现的协作式任务甚至能够在暂停前备份一部分调用栈。例如，Rust的async/await实现会保存在一个自动生成的结构体中保存需要的局部变量。通过保存一部分相关的调用栈，所有任务最终可以共享一个调用栈，这就使得内存消耗更节省，从而几乎可以创建任意数量的协作式任务，而不用担心OOM。
优缺点 缺点非常明显：任务之间的协作依赖于程序员，否则一个恶意的，或者存在bug的任务会一直执行下去，不主动让出CPU，导致其他任务饿死。因此，协作式多任务仅用于我们知晓每个任务的具体内容的时候，相对的，操作系统层面执行的第三方用户程序肯定不能用协作式调度（因为我们根本不知道这些程序的内容）。
然而，其性能和内存消耗由于抢占式多任务，因此协作式多任务通常用在单个程序中，尤其是与异步操作协同。
Rust中的async/await Rust语言提供了以async/await为基础的协作式多任务支持。在我们阐述async/await如何工作之前，我们首先需要理解Rust中的futures和异步编程。
Futures 一个future代表着一个可能还未准备好的值。例如一个由另一个任务计算出来的整数，或者一个正在下载中的文件。future的概念可以由下面的小例子阐述。
这个时序图展示了一个main函数首先从文件系统中读取一个文件，然后调用函数foo。这个过程重复了两次，第一次使用同步调用read_file，另一次是异步调用async_read_file。
通过同步调用，main函数需要等待文件读取完成，才能够执行foo；而通过异步调用，文件系统直接返回一个future，文件的读取异步地在后台运行，而main函数可以进一步执行foo（此时foo和文件的读取过程是并行的）。
Rust中的future 在Rust中，future的概念由Future这个trait实现，它的定义如下：
pub trait Future { type Output; fn poll(self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;&amp;#39;_&amp;gt;) -&amp;gt; Poll&amp;lt;Self::Output&amp;gt;; } 关联类型Output表示这个值的类型，例如async_read_file函数返回的future中，Output应当被设置成File。
poll方法用于检查该future中的值是否ready，它返回一个枚举类型Poll，其定义为：
pub enum Poll&amp;lt;T&amp;gt; { Ready(T), Pending, } 当值已经ready时，它被包在Ready里，否则返回Pending，表示值还在准备中。</description>
    </item>
    
    <item>
      <title>KMP与AC自动机</title>
      <link>https://zijiaw.github.io/posts/a6-kmpac/</link>
      <pubDate>Fri, 25 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a6-kmpac/</guid>
      <description>背景 最近又遇到了AC自动机，顺便复习了一下以前学过的KMP算法，两者其实具有很相似的结构，因此写篇文章总结一下。
KMP算法 问题 KMP算法解决字符串模式匹配问题，给定字符串s和p，计算s中出现p的位置，p即pattern。实际上单就这个问题而言，完全可以采用暴力匹配的方式，假设s的长度为m，p的长度为n，则暴力匹配的时间复杂度为O(mn)，相比之下，KMP算法的时间复杂度为O(m+n)。
s: abxcabcycabcc
p: cabcc
一些定义 先定义一些东西，方便后面阐释KMP算法背后的思想。
定义1：
 前缀 prefix：字符串a是字符串b的前缀，意味着a是b的子串，且出现在开头。 后缀 suffix：字符串a是字符串b的后缀，意味着a是b的子串，且出现为尾部。  &amp;quot;abc&amp;quot; is prefix of &amp;quot;abcdefg&amp;quot;
&amp;quot;efg&amp;quot; is suffix of &amp;quot;abcdefg&amp;quot;
在本文中，我们只考虑真子串的情况，虽然按照定义，任意字符串自身也是自身的前缀，但这对于KMP来说没有意义。现在，我们可以定义一个有趣的东西：给定字符串s，如果字符串r既是s的前缀，也是s的后缀，它就是s的公共前后缀。
如上图，当我们用模式p匹配字符串s时，如果s中多次出现的p存在重叠的情况，那么重叠部分其实就是p的一个公共前后缀。
然后我们可以发现公共前后缀的两个有趣的性质：
性质：
 如果字符串r1，r2是s的两个不同的公共前后缀，且len(r1)&amp;gt;len(r2)，则r2是r1的公共前后缀。 设s[0:i]的最长公共前后缀为s[0:k]，则s[0:k-1]必然是s[0:i-1]的一个公共前后缀。  这两个性质其实很trivial，读者自己画个图就能理解了，我懒得画图了，通过上面的性质1，我们很容易可以得出下面的结论：
引理1：
 设$r_1,r_2&amp;hellip;$是不同的字符串，$r_{i+1}$是$r_{i}$的最长公共前后缀，且$r_1$是$s$的最长公共前后缀，则$r_1,r_2&amp;hellip;$依次是$s$长度递减的所有的公共前后缀。  next数组 现在我们先不谈KMP算法，先计算一个数组next，它的长度等于字符串p的长度（即pattern的长度），这个数组是这么定义的：
定义2：
 next[k]是p[0:k]的最长公共前后缀的长度，也即p[0:k]的最长公共前后缀为p[0:next[k]-1]。特别地，next[0]=0。注意：这里的p[0:k]是闭区间。  vector&amp;lt;int&amp;gt; make_next(const string&amp;amp; p) { vector&amp;lt;int&amp;gt; next(p.size(), 0); int i = 0, j = 1; while (j &amp;lt; p.size()) { while (p[i] != p[j] &amp;amp;&amp;amp; i &amp;gt; 0) {// 根据引理1，按长度递减的顺序尝试所有可能的p[0:i-1]的公共前后缀  i = next[i - 1]; } if (p[i] == p[j]) {// 找到了匹配的p[i]和p[j]，则p[0:j]的最长公共前后缀为p[0:i]  next[j] = ++i; } else { next[j] = 0;// 否则为0  } j++; } return next; } 上面的代码计算next数组。实际上，循环中维护了如下的不变式：</description>
    </item>
    
    <item>
      <title>浅谈Golang性能优化</title>
      <link>https://zijiaw.github.io/posts/a5-time/</link>
      <pubDate>Sun, 09 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a5-time/</guid>
      <description>背景 最近在实习公司里负责一个性能比较敏感的服务（延迟P99在10ms），优化了之前出现的一些问题。这个服务之前在晚高峰期间的qps达到45w，接入了几个新功能后涨到50w+，线上是部署了约500个8核8G的实例，因此平均下来每个实例的qps在900~1100之间。
出现的问题如下：
  高负载下内存占用率极高，保持在7G左右，这样一抖动很容易OOM，现实是晚高峰期间都会有十几次OOM报警。
  延迟周期性抖动，用较高qps压测显示每两分钟延迟升高，CPU占用率抖动达到90%以上。在晚高峰下的表现为上游调用失败率（timeout）涨到1%左右。
  内存占用 对于第一个问题，我仔细阅读了该服务的代码，发现在每次请求的处理中需要多次使用哈希表来处理数据，而且逻辑上这个哈希表无法优化掉。原先的作者是如此构造这个哈希表的：在kv数量小于100的时候使用16个shard的二维slice来模拟，在数量大于100后转而把数据放进map[int]int中。最后使用Golang标准库的sync.Pool来复用这些map。
这么做粗看没什么问题，实际上问题大了，可能是之前服务的qps不高导致问题没有显现出来。通常我们使用sync.Pool是用于对象复用，也就是避免许多对象的重复内存分配，在对象用完以后把它Put到临时对象池里，尽量延迟它的回收，在下次申请同样的对象时从池子里拿。当然sync.Pool不是传统意义上的对象池，如果一些对象在一段时间内没有被重用，还是会被gc回收的。
但是在这里，sync.Pool回收的对象是数组和map，在我们放进去的时候自然是需要把数组每一个shard给清空的（这样才能在下一次的时候方便使用），map也是如此。那么此时被回收的对象可以理解为空的map和slice，真正的数据已经失引用了，等待gc回收。
go的内存回收有GOGC变量控制，默认为100，即堆内存达到上次回收后内存的两倍时触发自动gc，此外如果两分钟未触发过gc，则会强制触发。这个服务内存占用本身比较高，因此默认情况下很少会主动触发gc，一般都是2min一次强制gc。在这2min内这些失引用的对象就一直占用着本就捉襟见肘的内存，自然会导致内存占用极高，而且会随qps升高而升高。
于是这里我做的优化是用sync.Pool复用更基础的对象——哈希表的结点，在之前我们复用哈希表是无效的，因为哈希表清空后，其内部数据失引用。因此我自己实现一个简易版本的链式哈希表，这样其内部的结点可以手动管理，用于对象复用，定义如下：
type Node struct { key int val int next *Node } var NodePool *sync.Pool = &amp;amp;sync.Pool { New: func() interface{} { return &amp;amp;Node{} }, } 链式哈希表的实现就很简单了（不会的话自行google），在这里我使用固定的entry大小，不考虑哈希表的扩容（因为场景很单一，kv数量不会很多），在申请结点和free结点的时候从NodePool中取即可。经过这么优化，服务的内存占用骤降，从~7G一下子降到了4~5G，OOM的问题就完全解决了。
CPU性能 实际上这么实现后一段时间服务都没什么问题，无论是从内存还是延迟都有提升，直到今年五一上了一个新功能后，这个服务的qps涨了近30%，于是放假第一天晚上高峰期qps一下上到50w，错误率就涨到了2%，于是假期结束后又开始追查问题了。
经过压测可以观察到延迟的抖动以基本固定的2min间隔发生，那么基本上可以确定是gc导致的。回顾一下Golang的gc机制，可以参考这篇文章：
  清理终止阶段；
暂停程序，所有的处理器在这时会进入安全点（Safe point）；
如果当前垃圾收集循环是强制触发的，我们还需要处理还未被清理的内存管理单元；
  标记阶段；
将状态切换至 _GCmark、开启写屏障、用户程序协助（Mutator Assiste）并将根对象入队；
恢复执行程序，标记进程和用于协助的用户程序会开始并发标记内存中的对象，写屏障会将被覆盖的指针和新指针都标记成灰色，而所有新创建的对象都会被直接标记成黑色；
开始扫描根对象，包括所有 Goroutine 的栈、全局对象以及不在堆中的运行时数据结构，扫描 Goroutine 栈期间会暂停当前处理器；
依次处理灰色队列中的对象，将对象标记成黑色并将它们指向的对象标记成灰色；
使用分布式的终止算法检查剩余的工作，发现标记阶段完成后进入标记终止阶段；
  标记终止阶段；
暂停程序、将状态切换至 _GCmarktermination 并关闭辅助标记的用户程序；</description>
    </item>
    
    <item>
      <title>Reactor 模式</title>
      <link>https://zijiaw.github.io/posts/a4-reactor/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a4-reactor/</guid>
      <description>本文分析reactor模式，这是广泛应用于许多网络框架下的异步IO模式。
I/O I/O通常是现今计算机运行中最慢的操作，无论是读取文件还是网络传输。读取磁盘上的文件通常花费的时间在毫秒级，而一次内存的读取花费的时间在纳秒级。同样的，网络传输/磁盘读取的速度通常为以MB/s为单位，而内存存取速率通常以GB/s为单位。对于CPU来说，IO操作并不昂贵，但它通常会导致操作的延时。
Blocking I/O 在传统的阻塞式IO编程中，一次包含I/O请求的函数调用会阻塞整个线程，直到IO操作完成，这通常导致一个函数的操作时间过长。在这种模式下，显然单个线程无法同时处理多个网络请求。因此，在几十年前的服务器编程中，传统的方式是为每个到来的网络连接开启一个线程（或者从线程池中取一个空闲线程），这样单个请求的读取过程即使阻塞了一个线程，也不会影响其他请求的处理。
在这种模式下，任何有关IO的操作都会使得线程阻塞，这些操作包括读取socket，发送socket，读取文件，写文件，数据库请求等等。也就是说，在一次请求处理中，线程可能会阻塞很多次。在这期间实际上CPU是空闲的，而如果线程过多，线程的调度会加重CPU的负担——这是额外的消耗，特别是对于Linux这类重线程的操作系统，每个线程都耗费不小的内存空间，并带来上下文切换的开销。
Non-blocking I/O 除了阻塞式IO，大多数现代操作系统支持另一种获取资源的方式——非阻塞式I/O。在这种模式下，涉及IO的系统调用总是立即返回，而不等待数据读写完成。如果在调用时数据未就绪，则调用会返回一个预定义的值，表示当前数据未准备好。
例如，在Unix系统中，fcntl()函数可以用来控制一个已有的文件操作符，将其的读写模式改变为非阻塞（用O_NONBLOCK标志位）。一旦文件操作在非阻塞模式下，如果该文件还没有数据可读，则读操作会返回EAGAIN。
最基本的使用非阻塞IO的编程范式是在一个循环里不断地轮询相应接口，直到有数据返回——这被称为忙等待。下面的伪代码展示了这一逻辑：
resources = [socketA, socketB, pipeA];
while(!resources.isEmpty()) {
for(i = 0; i &amp;lt; resources.length; i++) {
resource = resources[i];
//try to read
var data = resource.read();
if(data === NO_DATA_AVAILABLE)
//there is no data to read at the moment
continue;
if(data === RESOURCE_CLOSED)
//the resource was closed, remove it from the list
resources.remove(i);
else
//some data was received, process it
consumeData(data);
}
}
你可以看到，这么简单的代码已经可以在单线程里面处理多个I/O资源的读写了，但还不够高效。事实上，在上面的例子中，忙等待会浪费很多CPU时间——因为大多数时候资源都是不可用的。我们希望等到资源可用的时候能够自动通知线程，而不是线程一直循环着浪费CPU资源。</description>
    </item>
    
    <item>
      <title>Paxos 算法</title>
      <link>https://zijiaw.github.io/posts/a3-paxos/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a3-paxos/</guid>
      <description>共识问题  分布式系统中的节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。基于消息传递通信模型的分布式系统，不可避免的会发生以下错误：进程可能会变慢、被杀死或者重启，消息可能会延迟、丢失、重复，我们不考虑可能出现恶意的消息篡改即拜占庭错误的情况。我们希望分布式系统中的多个进程能够就某些值或状态达成一致，即多个进程像是一个整体一样，了解彼此的一些状态，这就是共识。
Paxos算法解决的问题是：如何在一个可能发生上述异常的分布式系统中就某个值达成一致，保证不论发生以上任何异常，都不会破坏共识。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后就能到达一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“共识算法”以保证每个节点执行的指令一致。
Basic Paxos  Paxos算法有多种变体，但其基本思想均来源于basic paxos，这也是Lamport在论文中最先详细阐述的算法。在basic paxos中，我们对于共识的要求如下：
 安全性（坏事不会发生）  从头到尾只有一个值会被选择 当且仅当一个值被选择后，分布式系统中的服务器才能知道它被选择了   活性（好事最终会发生）  总会有某个被提出的值最终被选择 只要有一个值被选择，最终总会被服务器知道    在这里活性的条件是：只要分布式系统中超过半数的服务器还在工作，且其通信延时正常。
在此我们定义两类进程，这些进程可以运行在同一台机器上，也可以运行在不同的机器上：
 Proposer：接收来自客户端的请求，主动发起提议（propose some value）。 Acceptor：被动响应来自proposer的请求，也起到投票者的作用，每次投票称为accept，当多数acceptor接受了某个值，该值即被选择，同时保存被选择的值。  想要达成共识，一个最简单的想法是只设置一个acceptor，这样共识性显然是满足的，但是一旦该节点crash，整个系统就无法工作。因此采用quorum机制：设置多个acceptor（通常是奇数个），当过半的acceptor接受了一个值，就确定该值被选择。
现在我们考虑如何设计proposer和acceptor的运行机制，使得系统满足一致性。
  Accept First Value
每个acceptor简单地接受其得到的第一个值，这显然是不行的，因为网络具有延迟，可能多个并发的提议会交错到达各个acceptor，导致没有任何一个值被过半acceptor接受，系统不满足活性。
  Accept Every Value
如果接受所有值，也不可行，在这种情况下，随着多个提议到来，很可能会有多个值被选择。
  因此，在多acceptor的情况下，其必须能够接受多个值，但也需要在一些情况下拒绝一些值，例如：如果当前已经有一个值v1被选择了，那么后续所有的acceptor/proposer必须accept/propose相同的值——需要设计一套机制来保证。
首先，由于网络RPC的不稳定性，每个提议需要一个唯一递增的序列号（proposal number），即每个proposer在发送新的proposal时，需要保证其序号大于之前见过的所有序号，且不会与其他proposer重复。一个简单的做法是按照余数来递增选择序号。
如上图所示，当proposer为一个新的值选定序号后，首先向所有acceptor广播Prepare请求，</description>
    </item>
    
    <item>
      <title>What&#39;s JWT?</title>
      <link>https://zijiaw.github.io/posts/a2-jwt/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a2-jwt/</guid>
      <description>本文内容来自angular-jwt博客。
什么是JWT JWT全称为JSON Web Token，是一种跨域认证机制。一个JWT包含3部分，Header，Payload和Signature。通常经过编码后的jwt token会被放在header的authorization字段，以Bearer &amp;lt;token&amp;gt;的形式随请求传递，作为用户的临时凭证。
payload就是一个简单的json对象，比如下面这个payload包含了用户名，邮箱什么的，但实际上你可以放任何信息，比如转账流水之类的：
{
&amp;quot;name&amp;quot;: &amp;quot;John Doe&amp;quot;,
&amp;quot;email&amp;quot;: &amp;quot;john@johndoe.com&amp;quot;,
&amp;quot;admin&amp;quot;: true
}
虽然对于payload里面的内容没有限制，但是需要注意的是，JWT本身并没有被加密，也就是说不要把敏感信息放在payload里面，可能会受到黑客攻击。
payload里的内容需要由消息的接受者去校验，校验需要用到签名（signature），但是签名的种类很多，因此我们需要一个额外的字段来指出发送方使用的是哪种签名。这类信息就放在header里面，实际上header也是一个json，例如下面的东西：
{
&amp;quot;alg&amp;quot;: &amp;quot;RS256&amp;quot;,
&amp;quot;typ&amp;quot;: &amp;quot;JWT&amp;quot;
}
可以看到这个header指明了我们使用的签名算法是RS256，关于签名，后面还会讲，现在我们先看看如何通过签名来校验payload的合法性。
signature通常是用消息和一个密钥按照某种特定的签名算法生成的字符串，其认证过程如下：
 用户提交用户名和密码到认证服务器 服务器验证用户名和密码是正确的，则生成一个JWT，在payload里面注明用户的标识符，及其失效时间 服务器将payload和header放在一起，制作一个签名，三者组成一个JWT，返还给用户 用户拿到JWT后，后续每次请求服务都会把JWT带上 应用服务器后续用JWT作为用户的临时身份认证，不再需要用户再去输密码手动验证了  应用服务器按如下方式从JWT中确认用户信息：
 首先验证签名和内容是一致的，也就是说消息没有被篡改过 通过payload中的id来找到用户  这样，一个第三方的攻击者要么通过窃取用户名和密码来模仿用户，要么通过窃取认证服务器上的密钥才能破解这一过程。
签名机制保证一个无状态的服务器可以仅仅通过请求中带过来的一串JWT认证用户，而不需要用户每次都带上用户名和密码。
JWT这一机制的另一个显著的好处是可以将应用服务器和认证服务器分离，把认证功能单独抽出来，其他服务仅需要进行校验JWT即可，让其他服务更快更简洁。
更多细节 编码 我们来实际看看一个JWT长啥样，下面这个例子来自于jwt.io的在线JWT认证工具：
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ
你可以看到这个和之前我们说的好像不太一样，我们仔细观察一下，这串字符串是三个字符串以.组合在一起的，实际上第一部分是header，第二部分是payload，第三部分是signature：
Header: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9
Payload: eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9
Signature: TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ
但是这几个字符串依然不是人类可读的样子，这是因为我们对原始的内容做了编码，把二进制内容编码成全部ASCII字符可以避免很多网络传输中的编解码错误，例如不同的电脑，浏览器，文本编辑器使用不用的编码格式，比如utf8，utf16，ISO 8859-1等。我们为了避免这类问题，通常使用base64编码算法把任何字符串变成特定的一些字符。
不过还有一点不一样，JWT里面用的不是Base64，而是Base64Url，它和base64在某些字符上不太一样，以保证最终生成的字符串可以放在url中传输。
现在我们把payload部分输入到base64解码网站，比如base64decode，就能得到下面这串内容：
{
&amp;quot;sub&amp;quot;:&amp;quot;1234567890&amp;quot;,
&amp;quot;name&amp;quot;:&amp;quot;John Doe&amp;quot;, &amp;quot;admin&amp;quot;:true
}
这样就看到它的真面目了，实际上这一串操作是在请求中传json数据的基本操作。
现在我们再看看payload里面具体要传的东西是哪些，下面是一个具体的例子，这几个字段都是官方提供的可选字段：
{
&amp;quot;iss&amp;quot;: &amp;quot;Identifier of our Authentication Server&amp;quot;,
&amp;quot;iat&amp;quot;: 1504699136, &amp;quot;sub&amp;quot;: &amp;quot;github|353454354354353453&amp;quot;,
&amp;quot;exp&amp;quot;: 1504699256
}
 iss指的是issuing entity，即颁发认证的认证服务器 iat指的是此JWT的创建时间戳 sub指服务器生成的用户的唯一标识符 exp包含过期时间  这就是所谓的Bearer Token，bearer指持有者，也就是说持有此token的用户拥有sub定义的用户权限。</description>
    </item>
    
    <item>
      <title>Tokio Guide 中文翻译</title>
      <link>https://zijiaw.github.io/posts/a1-tokio/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zijiaw.github.io/posts/a1-tokio/</guid>
      <description>本文是阅读新版tokio文档，顺便翻译之作，啥时候读完啥时候写完咯。
Intruduction 本教程将一步步带你构建一个简化的Redis客户端和服务器。我们首先介绍Rust中异步编程的基本概念，而后实现Redis命令的一个子集。
Mini-Redis 安装完成Rust以后，请安装Mini-Redis服务器程序，这可以用来测试我们的程序：
cargo install mini-redis
你可以键入如下命令开始程序：
mini-redis-server
而后你可以另起一个命令行，用自带的客户端程序尝试获取键foo：
mini-redis-cli get foo
你将看到输出为nil。
Hello Tokio 引入 我们首先编写非常基础的Tokio应用，它将连接到我们的Mini-Redis服务器，插入键值对&amp;lt;hello, world&amp;gt;，而后我们用cli包来获取它，以验证正确性。
首先我们用carge创建新的Rust应用目录：
cargo new my-redis
cd my-redis
而后编辑Cargo.toml，添加依赖：
tokio = { version = &amp;quot;0.3&amp;quot;, features = [&amp;quot;full&amp;quot;] }
mini-redis = &amp;quot;0.3&amp;quot;
现在我们可以编辑main.rs，代码如下：
use mini_redis::{client, Result}; #[tokio::main] pub async fn main() -&amp;gt; Result&amp;lt;()&amp;gt; { // Open a connection to the mini-redis address.  let mut client = client::connect(&amp;#34;127.0.0.1:6379&amp;#34;).await?; // Set the key &amp;#34;hello&amp;#34; with value &amp;#34;world&amp;#34;  client.</description>
    </item>
    
  </channel>
</rss>
